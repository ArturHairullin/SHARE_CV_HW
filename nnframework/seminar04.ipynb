{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание\n",
    "\n",
    "## Данные о студенте\n",
    "\n",
    "1. **ФИО**: Хайруллин Артур Миннахматович\n",
    "2. **Факультет**: механико-математический\n",
    "3. **Курс**: 6\n",
    "4. **Группа**: 611\n",
    "\n",
    "## Замечания\n",
    "\n",
    "* Заполненный ноутбук необходимо сдать боту\n",
    "* Соблюдаем кодекс чести (по нулям и списавшему, и давшему списать)\n",
    "* Можно (и нужно!) применять для реализации только библиотеку **Numpy**\n",
    "* Ничего, крому Numpy, нельзя использовать для реализации \n",
    "* **Keras** используется только для тестирования Вашей реализации\n",
    "* Если какой-то из классов не проходит приведенные тесты, то соответствующее задание не оценивается\n",
    "* Возможно использование дополнительных (приватных) тестов\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание состоит из трёх частей:\n",
    "1. Реализация прямого вывода нейронной сети (5 баллов)\n",
    "2. Реализация градиентов по входу и распространения градиента по сети (5 баллов)\n",
    "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением параметров сети (10 баллов)\n",
    "\n",
    "Дополнительные баллы можно получить при реализации обучения сети со свёрточными слоями (10 баллов), с транспонированной свёрткой (10 баллов), дополнительного оптимизатора (5 баллов). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Реализация вывода собственной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Внимательно ознакомьтесь с интерфейсом слоя. Любой слой должен содержать как минимум три метода:\n",
    "- конструктор\n",
    "- прямой вывод \n",
    "- обратный вывод, производные по входу и по параметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'Layer'       \n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "    def backward(self, input_data):\n",
    "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
    "    \n",
    "    def grad_x(self, input_data):\n",
    "        pass\n",
    "    def grad_param(self, input_data):\n",
    "        return []\n",
    "    \n",
    "    def update_param(self, grads, learning_rate):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Ниже предствален интерфейс класса  Network. Обратите внимание на реализацию метода predict, который последовательно обрабатывает входные данные слой за слоем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, layers, loss=None):\n",
    "        self.name = 'Network'\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return self.predict(input_data)\n",
    "    \n",
    "    def grad_x(self, input_data, labels):\n",
    "        b = input_data.shape[0]\n",
    "        grad = []\n",
    "        for i in range(b):\n",
    "            current_input = input_data\n",
    "            g = np.eye(self.layers[0].grad_x(current_input)[i,...].shape[1])\n",
    "            for layer in self.layers:\n",
    "                g = layer.grad_x(current_input)[i]@g\n",
    "                current_input = layer.forward(current_input)\n",
    "            g = self.loss.grad_x(current_input, labels)[i]@g\n",
    "            grad.append(g)\n",
    "        return np.array(grad)\n",
    "    def grad_param(self, input_data, labels):\n",
    "        b = input_data.shape[0]\n",
    "        current_input = input_data.copy()\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            for p in range(len(params)):\n",
    "                for s in range(len(params[p])):\n",
    "                    a = []\n",
    "                    for i in range(b):\n",
    "                        a.append(layer.grad_x(current_input)[i]@params[p][s][i])\n",
    "                    params[p][s] = np.array(a)\n",
    "            params.append(layer.grad_param(current_input))\n",
    "            current_input = layer.forward(current_input)\n",
    "        for p in range(len(params)):\n",
    "            for s in range(len(params[p])):\n",
    "                a = []\n",
    "                for i in range(b):\n",
    "                    a.append(self.loss.grad_x(current_input, labels)[i]@params[p][s][i])\n",
    "                params[p][s] = np.array(a)\n",
    "        return params\n",
    "    def update(self, grad_list, learning_rate):\n",
    "        #print(grad_list)\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].update_param(grad_list[i], learning_rate)\n",
    "    def predict(self, input_data):\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)     \n",
    "        return current_input\n",
    "    \n",
    "    def calculate_loss(self, input_data, labels):\n",
    "        return self.loss.forward(self.predict(input_data), labels)\n",
    "    \n",
    "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
    "        grad_list = self.grad_param(input_data, labels)\n",
    "        self.update(grad_list, learning_rate)\n",
    "    \n",
    "    \n",
    "    def fit(self, trainX, trainY, validation_split=0.25, \n",
    "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
    "        \n",
    "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY, \n",
    "                                                          test_size=validation_split,\n",
    "                                                          random_state=42)\n",
    "        for epoch in range(nb_epoch):\n",
    "            #train one epoch\n",
    "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
    "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
    "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
    "                self.train_step(batch_x, batch_y, learning_rate)\n",
    "            #validate\n",
    "            val_accuracy = self.evaluate(val_x, val_y)\n",
    "            print('%d epoch: val %.2f' %(epoch+1, val_accuracy))\n",
    "            \n",
    "    def evaluate(self, testX, testY):\n",
    "        y_pred = np.argmax(self.predict(testX), axis=1)            \n",
    "        y_true = np.argmax(testY, axis=1)\n",
    "        val_accuracy = np.sum((y_pred == y_true))/(len(y_true))\n",
    "        return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Необходимо реализовать метод forward для вычисления следующих слоёв:\n",
    "\n",
    "- DenseLayer\n",
    "- ReLU\n",
    "- Softmax\n",
    "- FlattenLayer\n",
    "- MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорты\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None, random_state=42):\n",
    "        self.name = 'Dense'\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        if W_init is None or b_init is None:\n",
    "            self.W = np.random.RandomState(random_state).random((input_dim, output_dim))\n",
    "            self.b = np.zeros(output_dim, 'float32')\n",
    "        else:\n",
    "            self.W = W_init\n",
    "            self.b = b_init\n",
    "    def forward(self, input_data):\n",
    "        out = input_data@self.W + self.b\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        grad = []\n",
    "        b = input_data.shape[0]\n",
    "        for i in range(b):\n",
    "            grad.append(self.W.T.copy())\n",
    "        return np.array(grad)\n",
    "    def grad_b(self, input_data):\n",
    "        grad = []\n",
    "        b = input_data.shape[0]\n",
    "        for i in range(b):\n",
    "            grad.append(np.eye(self.output_dim))\n",
    "        return np.array(grad)\n",
    "    def grad_W(self, input_data):\n",
    "        grad = []\n",
    "        b = input_data.shape[0]\n",
    "        n = self.input_dim\n",
    "        w = self.output_dim\n",
    "        for l in range(b):\n",
    "            a = np.zeros((w, w*n))\n",
    "            for k in range(w):\n",
    "                for i in range(n):\n",
    "                    for j in range(w):\n",
    "                        if k == j:\n",
    "                            a[k][i*w + j] = input_data[l][i]\n",
    "            grad.append(a)\n",
    "        return np.array(grad)\n",
    "    \n",
    "    def update_W(self, grad, learning_rate):\n",
    "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
    "    \n",
    "    def update_b(self, grad,  learning_rate):\n",
    "        self.b -= learning_rate * np.mean(grad, axis=0)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_W(params_grad[0], learning_rate)\n",
    "        self.update_b(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
    "    \n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'ReLU'\n",
    "    def forward(self, input_data):\n",
    "        out = input_data.copy()\n",
    "        b = out.shape[0]\n",
    "        if out.ndim == 2:\n",
    "            n = out.shape[1]\n",
    "            for i in range(b):\n",
    "                for j in range(n):\n",
    "                    out[i][j] = max(0, out[i][j])\n",
    "        if out.ndim == 4:\n",
    "            c = out.shape[1]\n",
    "            h = out.shape[2]\n",
    "            w = out.shape[3]\n",
    "            for i in range(b):\n",
    "                for j in range(c):\n",
    "                    for k in range(h):\n",
    "                        for l in range(w):\n",
    "                            out[i][j][k][l] = max(0, out[i][j][k][l])\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        b = input_data.shape[0]\n",
    "        if input_data.ndim == 2:\n",
    "            n = input_data.shape[1]\n",
    "            a = np.zeros((b, n, n))\n",
    "            for k in range(b):\n",
    "                for i in range(n):\n",
    "                    if input_data[k][i] > 0:\n",
    "                        a[k][i][i] = 1\n",
    "                    if np.abs(input_data[k][i]) < 1e-12:\n",
    "                        a[k][i][i] = 0.5\n",
    "        if input_data.ndim == 4:\n",
    "            a = np.zeros((b, input_data[0].size, input_data[0].size))\n",
    "            for k in range(b):\n",
    "                for l in range(input_data.shape[1]):\n",
    "                    for i in range(input_data.shape[2]):\n",
    "                        for j in range(input_data.shape[3]):\n",
    "                            if input_data[k][l][i][j] > 0:\n",
    "                                a[k][l*input_data.shape[2]*input_data.shape[3] + i*input_data.shape[3] + j]\\\n",
    "                                [l*input_data.shape[2]*input_data.shape[3] + i*input_data.shape[3] + j] = 1\n",
    "                            if np.abs(input_data[k][l][i][j]) < 1e-12:\n",
    "                                a[k][l*input_data.shape[2]*input_data.shape[3] + i*input_data.shape[3] + j]\\\n",
    "                                [l*input_data.shape[2]*input_data.shape[3] + i*input_data.shape[3] + j] = 0.5\n",
    "        return a\n",
    "    \n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Softmax'\n",
    "    def forward(self, input_data):\n",
    "        out = []\n",
    "        b = input_data.shape[0]\n",
    "        for i in range(b):\n",
    "            p = np.sum(np.exp(input_data[i,...]-np.max(input_data[i,...])))\n",
    "            res = input_data[i,...].copy()\n",
    "            out.append(np.exp(res-np.max(input_data[i,...]))/p)\n",
    "        return np.array(out)\n",
    "    def grad_x(self, input_data):\n",
    "        b = input_data.shape[0]\n",
    "        n = input_data.shape[1]\n",
    "        grad = []\n",
    "        f = self.forward(input_data)\n",
    "        for k in range(b):\n",
    "            a = np.zeros((n,n))\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i==j:\n",
    "                        a[i][j] = f[k][i]*(1-f[k][i])\n",
    "                    else:\n",
    "                        a[i][j] = -f[k][i]*f[k][j]\n",
    "            grad.append(a)\n",
    "        return np.array(grad)\n",
    "\n",
    "\n",
    "class FlattenLayer(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = 'Flatten'\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        return input_data.reshape(len(input_data), -1)\n",
    "    def grad_x(self, input_data):\n",
    "        grad = []\n",
    "        b = input_data.shape[0]\n",
    "        a = input_data[0,...]\n",
    "        s = a.size\n",
    "        for i in range(b):\n",
    "            grad.append(np.eye(s))\n",
    "        return np.array(grad)\n",
    "\n",
    "\n",
    "class MaxPooling(Layer):\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.name = 'MaxPooling'\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "    def forward(self, input_data):\n",
    "        b = input_data.shape[0]\n",
    "        c = input_data.shape[1]\n",
    "        h = int((input_data.shape[2] - self.pool_size)/self.stride) + 1\n",
    "        w = int((input_data.shape[3] - self.pool_size)/self.stride) + 1\n",
    "        out = np.zeros((b, c, h, w))\n",
    "        for i in range(b):\n",
    "            for j in range(c):\n",
    "                for k in range(h):\n",
    "                    for l in range(w):\n",
    "                        out[i][j][k][l] = np.max(input_data[i, j, k*self.stride:(k*self.stride + self.pool_size),\\\n",
    "                                                            l*self.stride:(l*self.stride + self.pool_size)])\n",
    "        return out\n",
    "    def grad_x(self, input_data):\n",
    "        b = input_data.shape[0]\n",
    "        c = input_data.shape[1]\n",
    "        h = int((input_data.shape[2] - self.pool_size)/self.stride) + 1\n",
    "        w = int((input_data.shape[3] - self.pool_size)/self.stride) + 1\n",
    "        out = np.zeros((b, c*h*w, c*input_data.shape[2]*input_data.shape[3]))\n",
    "        for k in range(b):\n",
    "            for l in range(c):\n",
    "                for i in range(h):\n",
    "                    for j in range(w):\n",
    "                        ind = np.argmax(input_data[k, l, i*self.stride:(i*self.stride + self.pool_size),\\\n",
    "                                                j*self.stride:(j*self.stride + self.pool_size)])\n",
    "                        ik = int(ind/self.pool_size)\n",
    "                        jk = ind - ik*self.pool_size\n",
    "                        out[k][l*h*w + i*w + j][l*input_data.shape[2]*input_data.shape[3] + \\\n",
    "                                                (i*self.stride + ik)*input_data.shape[3] + j*self.stride + jk] = 1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Реализуйте теперь свёрточный слой и транспонированную свёртку  (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DLayer(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 padding='same', stride=1, K_init=None, b_init=None, random_state=42):\n",
    "        # padding: 'same' или 'valid'\n",
    "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "        # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
    "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "        self.name = 'Conv2D'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        if K_init is None or b_init is None:\n",
    "            self.kernel = np.random.RandomState(random_state).random((output_channels, input_channels, \\\n",
    "                                                                      kernel_size, kernel_size))\n",
    "            self.bias = np.zeros(output_channels, 'float32')\n",
    "        else:\n",
    "            self.kernel = K_init\n",
    "            self.bias = b_init\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "    def forward(self, input_data):\n",
    "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
    "        # Нужно заполнить Numpy-тензор out\n",
    "        if input_data.shape[1]!=self.input_channels:\n",
    "            raise ValueError('mismatched channels')\n",
    "        if self.padding == 'same':\n",
    "            shape = list(input_data.shape)\n",
    "            shape[2]+=2*int(self.kernel_size/2)\n",
    "            shape[3]+=2*int(self.kernel_size/2)\n",
    "            a = np.zeros(shape)\n",
    "            for k in range(shape[0]):\n",
    "                for l in range(shape[1]):\n",
    "                    for i in range(input_data.shape[2]):\n",
    "                        for j in range(input_data.shape[3]):\n",
    "                            a[k][l][i+int(self.kernel_size/2)]\\\n",
    "                            [j+int(self.kernel_size/2)] = input_data[k][l][i][j]\n",
    "            inputd = a.transpose((0, 2, 3, 1))\n",
    "        else:\n",
    "            inputd = input_data.transpose((0, 2, 3, 1))\n",
    "        out = self.conv(inputd, self.kernel.transpose((2, 3, 1, 0)), self.bias, self.stride)\n",
    "        return out.transpose((0, 3, 1, 2))\n",
    "    def conv(self, tens, kernel, bias, s):\n",
    "        kw = kernel.shape[1]\n",
    "        h = tens.shape[1]\n",
    "        oh = int((h-kw)/s) + 1\n",
    "        w = tens.shape[2]\n",
    "        ow = int((w-kw)/s) + 1\n",
    "        batch = tens.shape[0]\n",
    "        cout = kernel.shape[3]\n",
    "        cin = tens.shape[3]\n",
    "        p = list(range(batch))\n",
    "        for b in range(batch):\n",
    "            p[b] = []\n",
    "            for j in range(cout):\n",
    "                res = np.zeros(oh*ow)\n",
    "                for i in range(cin):\n",
    "                    kerm = self.initm(tens, kernel, s, i, j)\n",
    "                    tv = self.initv(tens, b, i)\n",
    "                    r = kerm @ tv\n",
    "                    res = res + r\n",
    "                res = res + self.initb(tens, kernel, bias, s, j)\n",
    "                res = np.reshape(res,(oh,-1))\n",
    "                p[b].append(res)\n",
    "        return np.transpose(np.array(p), (0, 2, 3, 1))\n",
    "    def initm(self, tens, kernel, s, i, j):\n",
    "        kw = kernel.shape[1]\n",
    "        h = tens.shape[1]\n",
    "        oh = int((h-kw)/s) + 1\n",
    "        w = tens.shape[2]\n",
    "        ow = int((w-kw)/s) + 1\n",
    "        kerm = np.zeros((oh*ow, h*w))\n",
    "        for k in range(oh):\n",
    "            lm = k*s*w\n",
    "            for l in range(ow):\n",
    "                for n in range(kw):\n",
    "                    for p in range(kw):\n",
    "                        kerm[k*ow + l][lm + l*s + n*w + p] = kernel[n][p][i][j]\n",
    "        return kerm\n",
    "    def initv(self, tens, i, j):\n",
    "        h = tens.shape[1]\n",
    "        w = tens.shape[2]\n",
    "        tv = np.zeros(h*w)\n",
    "        for k in range(h):\n",
    "            for l in range(w):\n",
    "                tv[k*w + l] = tens[i][k][l][j]\n",
    "        return tv\n",
    "    def initb(self, tens, kernel, bias, s, j):\n",
    "        kw = kernel.shape[1]\n",
    "        h = tens.shape[1]\n",
    "        oh = int((h-kw)/s) + 1\n",
    "        w = tens.shape[2]\n",
    "        ow = int((w-kw)/s) + 1\n",
    "        bv = np.ones(oh*ow)\n",
    "        bv = bias[j]*bv\n",
    "        return bv        \n",
    "    def grad_x(self, input_data):\n",
    "        grad = []\n",
    "        b = input_data.shape[0]\n",
    "        if self.padding == 'same':\n",
    "            shape = list(input_data.shape)\n",
    "            shape[2]+=2*int(self.kernel_size/2)\n",
    "            shape[3]+=2*int(self.kernel_size/2)\n",
    "            a1 = np.zeros(shape)\n",
    "            for k in range(shape[0]):\n",
    "                for l in range(shape[1]):\n",
    "                    for i in range(input_data.shape[2]):\n",
    "                        for j in range(input_data.shape[3]):\n",
    "                            a1[k][l][i+int(self.kernel_size/2)]\\\n",
    "                            [j+int(self.kernel_size/2)] = input_data[k][l][i][j]\n",
    "            inputd = a1.transpose((0, 2, 3, 1))\n",
    "            kw = self.kernel_size\n",
    "            h = input_data.shape[2]\n",
    "            w = input_data.shape[3]\n",
    "            a = np.zeros((h*w, (h+kw-1)*(w+kw-1)))\n",
    "            for i in range(h):\n",
    "                for j in range(w):\n",
    "                    a[i*w+j][w+kw-1+int(kw/2)+i*(w+kw-1)+j] = 1\n",
    "            fres = []\n",
    "            for j in range(self.output_channels):\n",
    "                res = []\n",
    "                for i in range(self.input_channels):\n",
    "                    res.append(self.initm(inputd, self.kernel.transpose((2, 3, 1, 0)), self.stride, i, j)@a.T)\n",
    "                res = np.array(res).transpose((1,0,2))\n",
    "                res = res.reshape(res.shape[0],-1)\n",
    "                fres.append(res)\n",
    "            fres = np.array(fres)\n",
    "            fres = np.array(fres.reshape(fres.shape[0]*fres.shape[1],-1))\n",
    "            for i in range(b):\n",
    "                grad.append(fres.copy())\n",
    "            return np.array(grad)\n",
    "        else:\n",
    "            inputd = input_data.transpose((0, 2, 3, 1))\n",
    "            fres = []\n",
    "            for j in range(self.output_channels):\n",
    "                res = []\n",
    "                for i in range(self.input_channels):\n",
    "                    res.append(self.initm(inputd, self.kernel.transpose((2, 3, 1, 0)), self.stride, i, j))\n",
    "                res = np.array(res).transpose((1,0,2))\n",
    "                res = res.reshape(res.shape[0],-1)\n",
    "                fres.append(res)\n",
    "            fres = np.array(fres)\n",
    "            fres = np.array(fres.reshape(fres.shape[0]*fres.shape[1],-1))\n",
    "            for i in range(b):\n",
    "                grad.append(fres.copy())\n",
    "            return np.array(grad)\n",
    "    def grad_kernel(self, input_data):\n",
    "        grad = []\n",
    "        kw = self.kernel_size\n",
    "        b = input_data.shape[0]\n",
    "        h = input_data.shape[2]\n",
    "        oh = int((h-kw)/self.stride) + 1\n",
    "        w = input_data.shape[3]\n",
    "        ow = int((w-kw)/self.stride) + 1\n",
    "        if self.padding == 'same':\n",
    "            shape = list(input_data.shape)\n",
    "            shape[2]+=2*int(self.kernel_size/2)\n",
    "            shape[3]+=2*int(self.kernel_size/2)\n",
    "            inputd = np.zeros(shape)\n",
    "            for k in range(shape[0]):\n",
    "                for l in range(shape[1]):\n",
    "                    for i in range(input_data.shape[2]):\n",
    "                        for j in range(input_data.shape[3]):\n",
    "                            inputd[k][l][i+int(self.kernel_size/2)]\\\n",
    "                            [j+int(self.kernel_size/2)] = input_data[k][l][i][j]\n",
    "            for c in range(b):\n",
    "                a = np.zeros((self.output_channels*h*w, kw*kw*self.output_channels*self.input_channels))\n",
    "                for i in range(self.output_channels):\n",
    "                    for j in range(h):\n",
    "                        for k in range(w):\n",
    "                            for l in range(self.input_channels):\n",
    "                                for q in range(kw):\n",
    "                                    for p in range(kw):\n",
    "                                        a[i*h*w + j*w + k][i*kw*kw*self.input_channels + l*kw*kw + q*kw + p] = \\\n",
    "                                        inputd[c][l][j+q][k+p]\n",
    "                grad.append(a.copy())\n",
    "        else:\n",
    "            for c in range(b):\n",
    "                a = np.zeros((self.output_channels*oh*ow, kw*kw*self.output_channels*self.input_channels))\n",
    "                for i in range(self.output_channels):\n",
    "                    for j in range(oh):\n",
    "                        for k in range(ow):\n",
    "                            for l in range(self.input_channels):\n",
    "                                for q in range(kw):\n",
    "                                    for p in range(kw):\n",
    "                                        a[i*oh*ow + j*ow + k][i*kw*kw*self.input_channels + l*kw*kw + q*kw + p] = \\\n",
    "                                        input_data[c][l][j*self.stride+q][k*self.stride+p]\n",
    "                grad.append(a.copy())\n",
    "        return np.array(grad)\n",
    "    def grad_bias(self, input_data):\n",
    "        grad = []\n",
    "        kw = self.kernel_size\n",
    "        b = input_data.shape[0]\n",
    "        h = input_data.shape[2]\n",
    "        oh = int((h-kw)/self.stride) + 1\n",
    "        w = input_data.shape[3]\n",
    "        ow = int((w-kw)/self.stride) + 1\n",
    "        if self.padding == 'same':\n",
    "            a = np.zeros((self.output_channels*h*w, self.output_channels))\n",
    "            for i in range(self.output_channels):\n",
    "                for j in range(h*w):\n",
    "                    a[i*h*w + j][i] = 1\n",
    "        else:\n",
    "            a = np.zeros((self.output_channels*oh*ow, self.output_channels))\n",
    "            for i in range(self.output_channels):\n",
    "                for j in range(oh*ow):\n",
    "                    a[i*oh*ow + j][i] = 1\n",
    "        for i in range(b):\n",
    "            grad.append(a.copy())\n",
    "        return np.array(grad)\n",
    "    def update_kernel(self, grad, learning_rate):\n",
    "        self.kernel -= learning_rate * np.mean(grad, axis=0).reshape(self.kernel.shape)\n",
    "    \n",
    "    def update_bias(self, grad,  learning_rate):\n",
    "        self.bias -= learning_rate * np.mean(grad, axis=0)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_kernel(params_grad[0], learning_rate)\n",
    "        self.update_bias(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_kernel(input_data), self.grad_bias(input_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DTrLayer(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 padding=0, stride=1, K_init=None, b_init=None, random_state=42):      \n",
    "        # padding: число (сколько отрезать от модифицированной входной карты)\n",
    "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "        # stride - одно число (коэффициент расширения)\n",
    "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "        self.name = 'Conv2DTr'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        if K_init is None or b_init is None:\n",
    "            self.kernel = np.random.RandomState(random_state).random((kernel_size, kernel_size,\\\n",
    "                                                                      input_channels, output_channels))\n",
    "            self.bias = np.zeros(output_channels, 'float32')\n",
    "        else:\n",
    "            self.kernel = K_init\n",
    "            self.bias = b_init\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "    def forward(self, input_data):\n",
    "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
    "        # Нужно заполнить Numpy-тензор out \n",
    "        if input_data.shape[1]!=self.input_channels:\n",
    "            raise ValueError('mismatched channels')\n",
    "        shape = list(input_data.shape)\n",
    "        shape[2] = (shape[2]-1)*self.stride + 2*(self.kernel_size-1-self.padding) + 1\n",
    "        shape[3] = (shape[3]-1)*self.stride + 2*(self.kernel_size-1-self.padding) + 1\n",
    "        a = np.zeros(shape)\n",
    "        for k in range(shape[0]):\n",
    "            for l in range(shape[1]):\n",
    "                for i in range(input_data.shape[2]):\n",
    "                    for j in range(input_data.shape[3]):\n",
    "                        a[k][l][i*self.stride + self.kernel_size-1-self.padding]\\\n",
    "                        [j*self.stride + self.kernel_size-1-self.padding] = input_data[k][l][i][j]\n",
    "        inputd = a.transpose((0, 2, 3, 1))\n",
    "        out = self.conv(inputd, self.kernel, self.bias, 1)\n",
    "        return out.transpose((0, 3, 1, 2))\n",
    "    def conv(self, tens, kernel, bias, s):\n",
    "        kw = kernel.shape[1]\n",
    "        h = tens.shape[1]\n",
    "        oh = int((h-kw)/s) + 1\n",
    "        w = tens.shape[2]\n",
    "        ow = int((w-kw)/s) + 1\n",
    "        batch = tens.shape[0]\n",
    "        cout = kernel.shape[3]\n",
    "        cin = tens.shape[3]\n",
    "        p = list(range(batch))\n",
    "        for b in range(batch):\n",
    "            p[b] = []\n",
    "            for j in range(cout):\n",
    "                res = np.zeros(oh*ow)\n",
    "                for i in range(cin):\n",
    "                    kerm = self.initm(tens, kernel, s, i, j)\n",
    "                    tv = self.initv(tens, b, i)\n",
    "                    r = kerm @ tv\n",
    "                    res = res + r\n",
    "                res = res + self.initb(tens, kernel, bias, s, j)\n",
    "                res = np.reshape(res,(oh,-1))\n",
    "                p[b].append(res)\n",
    "        return np.transpose(np.array(p), (0, 2, 3, 1))\n",
    "    def initm(self, tens, kernel, s, i, j):\n",
    "        kw = kernel.shape[1]\n",
    "        h = tens.shape[1]\n",
    "        oh = int((h-kw)/s) + 1\n",
    "        w = tens.shape[2]\n",
    "        ow = int((w-kw)/s) + 1\n",
    "        kerm = np.zeros((oh*ow, h*w))\n",
    "        for k in range(oh):\n",
    "            lm = k*s*w\n",
    "            for l in range(ow):\n",
    "                for n in range(kw):\n",
    "                    for p in range(kw):\n",
    "                        kerm[k*ow + l][lm + l*s + n*w + p] = kernel[n][p][i][j]\n",
    "        return kerm\n",
    "    def initv(self, tens, i, j):\n",
    "        h = tens.shape[1]\n",
    "        w = tens.shape[2]\n",
    "        tv = np.zeros(h*w)\n",
    "        for k in range(h):\n",
    "            for l in range(w):\n",
    "                tv[k*w + l] = tens[i][k][l][j]\n",
    "        return tv\n",
    "    def initb(self, tens, kernel, bias, s, j):\n",
    "        kw = kernel.shape[1]\n",
    "        h = tens.shape[1]\n",
    "        oh = int((h-kw)/s) + 1\n",
    "        w = tens.shape[2]\n",
    "        ow = int((w-kw)/s) + 1\n",
    "        bv = np.ones(oh*ow)\n",
    "        bv = bias[j]*bv\n",
    "        return bv \n",
    "    def grad_x(self, input_data):\n",
    "        grad = []\n",
    "        b = input_data.shape[0]\n",
    "        shape = list(input_data.shape)\n",
    "        shape[2] = (shape[2]-1)*self.stride + 2*(self.kernel_size-1-self.padding) + 1\n",
    "        shape[3] = (shape[3]-1)*self.stride + 2*(self.kernel_size-1-self.padding) + 1\n",
    "        a = np.zeros(shape)\n",
    "        for k in range(shape[0]):\n",
    "            for l in range(shape[1]):\n",
    "                for i in range(input_data.shape[2]):\n",
    "                    for j in range(input_data.shape[3]):\n",
    "                        a[k][l][i*self.stride + self.kernel_size-1-self.padding]\\\n",
    "                        [j*self.stride + self.kernel_size-1-self.padding] = input_data[k][l][i][j]\n",
    "        inputd = a.transpose((0, 2, 3, 1))\n",
    "        kw = self.kernel_size\n",
    "        h1 = shape[2]\n",
    "        w1 = shape[3]\n",
    "        h = input_data.shape[2]\n",
    "        w = input_data.shape[3]\n",
    "        a1 = np.zeros((h*w, h1*w1))\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                a1[i*w+j][((w-1)*self.stride + 2*(self.kernel_size-1-self.padding) + 1)*(self.kernel_size-1-self.padding) + \\\n",
    "                          (self.kernel_size-1-self.padding) + j*self.stride + \\\n",
    "                          i*self.stride*((w-1)*self.stride + 2*(self.kernel_size-1-self.padding) + 1)] = 1\n",
    "        fres = []\n",
    "        for j in range(self.output_channels):\n",
    "            res = []\n",
    "            for i in range(self.input_channels):\n",
    "                res.append(self.initm(inputd, self.kernel, 1, i, j)@a1.T)\n",
    "            res = np.array(res).transpose((1,0,2))\n",
    "            res = res.reshape(res.shape[0],-1)\n",
    "            fres.append(res)\n",
    "        fres = np.array(fres)\n",
    "        fres = np.array(fres.reshape(fres.shape[0]*fres.shape[1],-1))\n",
    "        for i in range(b):\n",
    "            grad.append(fres.copy())\n",
    "        return np.array(grad)\n",
    "    def grad_kernel(self, input_data):\n",
    "        grad = []\n",
    "        kw = self.kernel_size\n",
    "        b = input_data.shape[0]\n",
    "        shape = list(input_data.shape)\n",
    "        shape[2] = (shape[2]-1)*self.stride + 2*(self.kernel_size-1-self.padding) + 1\n",
    "        shape[3] = (shape[3]-1)*self.stride + 2*(self.kernel_size-1-self.padding) + 1\n",
    "        inputd = np.zeros(shape)\n",
    "        h = (input_data.shape[2]-1)*self.stride - 2*self.padding + kw\n",
    "        w = (input_data.shape[3]-1)*self.stride - 2*self.padding + kw\n",
    "        for k in range(shape[0]):\n",
    "            for l in range(shape[1]):\n",
    "                for i in range(input_data.shape[2]):\n",
    "                    for j in range(input_data.shape[3]):\n",
    "                        inputd[k][l][i*self.stride + self.kernel_size-1-self.padding]\\\n",
    "                        [j*self.stride + self.kernel_size-1-self.padding] = input_data[k][l][i][j]\n",
    "        for c in range(b):\n",
    "            a = np.zeros((self.output_channels*h*w, kw*kw*self.output_channels*self.input_channels))\n",
    "            for i in range(self.output_channels):\n",
    "                for j in range(h):\n",
    "                    for k in range(w):\n",
    "                        for l in range(self.input_channels):\n",
    "                            for q in range(kw):\n",
    "                                for p in range(kw):\n",
    "                                    a[i*h*w + j*w + k][i*kw*kw*self.input_channels + l*kw*kw + q*kw + p] = \\\n",
    "                                    inputd[c][l][j+q][k+p]\n",
    "            grad.append(a.copy())\n",
    "        return np.array(grad)\n",
    "    def grad_bias(self, input_data):\n",
    "        grad = []\n",
    "        kw = self.kernel_size\n",
    "        b = input_data.shape[0]\n",
    "        h = (input_data.shape[2]-1)*self.stride - 2*self.padding + kw\n",
    "        w = (input_data.shape[3]-1)*self.stride - 2*self.padding + kw\n",
    "        a = np.zeros((self.output_channels*h*w, self.output_channels))\n",
    "        for i in range(self.output_channels):\n",
    "            for j in range(h*w):\n",
    "                a[i*h*w + j][i] = 1\n",
    "        for i in range(b):\n",
    "            grad.append(a.copy())\n",
    "        return np.array(grad)\n",
    "    def update_kernel(self, grad, learning_rate):\n",
    "        self.kernel -= learning_rate * np.mean(grad, axis=0).reshape(self.kernel.shape)\n",
    "    \n",
    "    def update_bias(self, grad,  learning_rate):\n",
    "        self.bias -= learning_rate * np.mean(grad, axis=0)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_kernel(params_grad[0], learning_rate)\n",
    "        self.update_bias(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_kernel(input_data), self.grad_bias(input_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DLayer1(Layer):\n",
    "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3, \n",
    "                 padding='valid', stride=1, K_init=None, b_init=None, random_state=42):\n",
    "        # padding: 'same' или 'valid'\n",
    "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
    "        # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
    "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
    "        self.name = 'Conv2D'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        if K_init is None or b_init is None:\n",
    "            self.kernel = np.random.RandomState(random_state).random((output_channels, input_channels, \\\n",
    "                                                                      kernel_size, kernel_size))\n",
    "            self.bias = np.zeros(output_channels, 'float32')\n",
    "        else:\n",
    "            self.kernel = K_init\n",
    "            self.bias = b_init\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "    def forward(self, input_data):\n",
    "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
    "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
    "        # Нужно заполнить Numpy-тензор out\n",
    "        if input_data.shape[1]!=self.input_channels:\n",
    "            raise ValueError('mismatched channels')\n",
    "        oshape = (input_data.shape[0], self.output_channels, int((input_data.shape[2] - self.kernel_size)/self.stride)+1, \\\n",
    "                  int((input_data.shape[3] - self.kernel_size)/self.stride)+1) \n",
    "        out = np.zeros(oshape)\n",
    "        for k in range(oshape[0]):\n",
    "            for l in range(oshape[1]):\n",
    "                for i in range(oshape[2]):\n",
    "                    for j in range(oshape[3]):\n",
    "                        for lk in range(self.input_channels):\n",
    "                            for ik in range(self.kernel_size):\n",
    "                                for jk in range(self.kernel_size):\n",
    "                                    out[k][l][i][j]+=input_data[k][lk][i*self.stride+ik][j*self.stride+jk]*\\\n",
    "                                    self.kernel[l][lk][ik][jk]\n",
    "                        out[k][l][i][j]+=self.bias[l]\n",
    "        return out\n",
    "            \n",
    "    def grad_x(self, input_data):\n",
    "        oshape = (input_data.shape[0], self.output_channels, int((input_data.shape[2] - self.kernel_size)/self.stride)+1, \\\n",
    "                  int((input_data.shape[3] - self.kernel_size)/self.stride)+1) \n",
    "        out = np.zeros((oshape[0], oshape[1]*oshape[2]*oshape[3], input_data[0].size))\n",
    "        for k in range(oshape[0]):\n",
    "            for l in range(oshape[1]):\n",
    "                for i in range(oshape[2]):\n",
    "                    for j in range(oshape[3]):\n",
    "                        for lk in range(self.input_channels):\n",
    "                            for ik in range(self.kernel_size):\n",
    "                                for jk in range(self.kernel_size):\n",
    "                                    out[k][l*oshape[2]*oshape[3] + i*oshape[3] + j]\\\n",
    "                                    [lk*input_data[0][0].size + (i*self.stride+ik)*input_data[0][0][0].size + \\\n",
    "                                     (j*self.stride+jk)] = self.kernel[l][lk][ik][jk]\n",
    "        return out\n",
    "    def grad_kernel(self, input_data):\n",
    "        oshape = (input_data.shape[0], self.output_channels, int((input_data.shape[2] - self.kernel_size)/self.stride)+1, \\\n",
    "                  int((input_data.shape[3] - self.kernel_size)/self.stride)+1) \n",
    "        out = np.zeros((oshape[0], oshape[1]*oshape[2]*oshape[3], self.kernel.size))\n",
    "        for k in range(oshape[0]):\n",
    "            for l in range(oshape[1]):\n",
    "                for i in range(oshape[2]):\n",
    "                    for j in range(oshape[3]):\n",
    "                        for lk in range(self.input_channels):\n",
    "                            for ik in range(self.kernel_size):\n",
    "                                for jk in range(self.kernel_size):\n",
    "                                    out[k][l*oshape[2]*oshape[3] + i*oshape[3] + j]\\\n",
    "                                    [l*self.kernel[0].size + lk*self.kernel[0][0].size +\\\n",
    "                                     ik*self.kernel_size + jk] = input_data[k][lk][i*self.stride + ik][j*self.stride + jk]\n",
    "        return out\n",
    "    def grad_bias(self, input_data):\n",
    "        grad = []\n",
    "        kw = self.kernel_size\n",
    "        b = input_data.shape[0]\n",
    "        h = input_data.shape[2]\n",
    "        oh = int((h-kw)/self.stride) + 1\n",
    "        w = input_data.shape[3]\n",
    "        ow = int((w-kw)/self.stride) + 1\n",
    "        if self.padding == 'same':\n",
    "            a = np.zeros((self.output_channels*h*w, self.output_channels))\n",
    "            for i in range(self.output_channels):\n",
    "                for j in range(h*w):\n",
    "                    a[i*h*w + j][i] = 1\n",
    "        else:\n",
    "            a = np.zeros((self.output_channels*oh*ow, self.output_channels))\n",
    "            for i in range(self.output_channels):\n",
    "                for j in range(oh*ow):\n",
    "                    a[i*oh*ow + j][i] = 1\n",
    "        for i in range(b):\n",
    "            grad.append(a.copy())\n",
    "        return np.array(grad)\n",
    "    def update_kernel(self, grad, learning_rate):\n",
    "        self.kernel -= learning_rate * np.mean(grad, axis=0).reshape(self.kernel.shape)\n",
    "    \n",
    "    def update_bias(self, grad,  learning_rate):\n",
    "        self.bias -= learning_rate * np.mean(grad, axis=0)\n",
    "        \n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_kernel(params_grad[0], learning_rate)\n",
    "        self.update_bias(params_grad[1], learning_rate)\n",
    "    \n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_kernel(input_data), self.grad_bias(input_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Теперь настало время теста. \n",
    "#### Если вы всё сделали правильно, то запустив следующие ячейки у вас должна появиться надпись: Test PASSED\n",
    "\n",
    "Переходить к дальнейшим заданиям не имеем никакого смысла, пока вы не добьётесь прохождение теста\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    " \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    " \n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "\n",
    "print(keras.__version__)\n",
    "\n",
    "def get_keras_model():\n",
    "    input_image = Input(shape=(28, 28, 1))\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(input_image)\n",
    "    flatten = Flatten()(pool1)\n",
    "    dense1 = Dense(10, activation='softmax')(flatten)\n",
    "    model = Model(inputs=input_image, outputs=dense1)\n",
    "\n",
    "    from keras.optimizers import Adam, SGD\n",
    "    sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train.transpose((0,2,3,1)), Y_train, validation_split=0.25, \n",
    "                        batch_size=32, epochs=2, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_our_model(keras_model):\n",
    "    maxpool = MaxPooling()\n",
    "    flatten = FlattenLayer()\n",
    "    dense = DenseLayer(196, 10, W_init=keras_model.get_weights()[0],\n",
    "                       b_init=keras_model.get_weights()[1])\n",
    "    softmax = Softmax()\n",
    "    net = Network([maxpool, flatten, dense, softmax])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.5729 - accuracy: 0.8469 - val_loss: 0.3788 - val_accuracy: 0.8943\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.3758 - accuracy: 0.8931 - val_loss: 0.3438 - val_accuracy: 0.9019\n",
      "(196, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "keras_model = get_keras_model()\n",
    "print(keras_model.get_weights()[0].shape)\n",
    "print(keras_model.get_weights()[1].shape)\n",
    "our_model = get_our_model(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 732us/step\n"
     ]
    }
   ],
   "source": [
    "keras_prediction = keras_model.predict(X_test.transpose((0,2,3,1)))\n",
    "our_model_prediction = our_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "if np.sum(np.abs(keras_prediction - our_model_prediction)) < 0.01:\n",
    "    print('Test PASSED')\n",
    "else:\n",
    "    print('Something went wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Вычисление производных по входу для слоёв нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании запрещено использовать численные формулы для вычисления производных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Реализуйте метод forward для класса CrossEntropy\n",
    "Напоминание: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
    "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self, eps=0.00001):\n",
    "        self.name = 'CrossEntropy'\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, input_data, labels):\n",
    "        out = []\n",
    "        b = input_data.shape[0]\n",
    "        for i in range(b):\n",
    "            out.append(-np.sum(labels[i,...]*np.log((input_data[i,...]+1e-12))))\n",
    "        return np.array(out)\n",
    "    \n",
    "    def calculate_loss(self,input_data, labels):\n",
    "        return self.forward(input_data, labels)\n",
    "    \n",
    "    def grad_x(self, input_data, lables):\n",
    "        grad = []\n",
    "        b = input_data.shape[0]\n",
    "        for i in range(b):\n",
    "            grad.append(-lables[i,...]/(input_data[i,...]+1e-12))\n",
    "        return np.array(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  Реализуйте метод grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_net(net, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x-delta, labels)) / (2*eps)\n",
    "        right_answer.append(diff)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_net(net):\n",
    "    x = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "    labels = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
    "    num_grad = numerical_diff_net(net, x, labels)\n",
    "    grad = net.grad_x(x, labels)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "        \n",
    "loss = CrossEntropy()\n",
    "test_net(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3  Реализуйте метод grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3)\n",
      "(2, 3, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_layer(layer, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (layer.forward(x + delta) - layer.forward(x-delta)) / (2*eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layer(layer):\n",
    "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
    "    num_grad = numerical_diff_layer(layer, x)\n",
    "    grad = layer.grad_x(x)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "        \n",
    "layer = Softmax()\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4  Реализуйте метод grad_x для классов ReLU и DenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3)\n",
      "(2, 3, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = ReLU()\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 3)\n",
      "(2, 4, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = DenseLayer(3,4)\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 (4 балла) Для класса Network реализуйте метод grad_x, который должен реализовывать взятие производной от лосса по входу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy())\n",
    "test_net(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n",
      "(10, 784)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_net1(net, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            for k in range(x.shape[3]):\n",
    "                delta = np.zeros((x.shape[1], x.shape[2], x.shape[3]))\n",
    "                delta[i][j][k] = eps\n",
    "                diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x-delta, labels)) / (2*eps)\n",
    "                right_answer.append(diff)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_net1(net):\n",
    "    x = X_train[::6000]\n",
    "    labels = Y_train[::6000]\n",
    "    num_grad = numerical_diff_net1(net, x, labels)\n",
    "    grad = net.grad_x(x, labels)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "net = Network([Conv2DLayer(padding='valid', stride=3, input_channels=1, output_channels=1),\\\n",
    "               Conv2DTrLayer(stride=2, input_channels=1, output_channels=1), \\\n",
    "               FlattenLayer(), DenseLayer(361, 10), ReLU(), DenseLayer(10, 10), Softmax()], loss=CrossEntropy())\n",
    "test_net1(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  Реализуйте функции grad_b и grad_W. При подготовке теста grad_W предполагается, что W является отномерным вектором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n",
      "(2, 4, 4)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_b(input_size, output_size, b, W, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(b)):\n",
    "        delta = np.zeros(b.shape)\n",
    "        delta[i] = eps\n",
    "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b+delta)\n",
    "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b-delta)\n",
    "        diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_b():\n",
    "    input_size = 3\n",
    "    output_size = 4 \n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((output_size,))\n",
    "    x = np.random.random((2, input_size))\n",
    "    \n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_b(x)\n",
    "\n",
    "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "\n",
    "test_grad_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 12)\n",
      "(2, 4, 12)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_W(input_size, output_size, b, W, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            delta = np.zeros(W.shape)\n",
    "            delta[i, j] = eps\n",
    "            dense1 = DenseLayer(input_size, output_size, W_init=W+delta, b_init=b)\n",
    "            dense2 = DenseLayer(input_size, output_size, W_init=W-delta, b_init=b)\n",
    "            diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
    "            right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_W():\n",
    "    input_size = 3\n",
    "    output_size = 4 \n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((4,))\n",
    "    x = np.random.random((2, input_size))\n",
    "        \n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_W(x)\n",
    "\n",
    "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "\n",
    "test_grad_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12)\n",
      "(2, 12)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad(input_size, output_size, b, W, x, l):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            delta = np.zeros(W.shape)\n",
    "            delta[i, j] = eps\n",
    "            net1 = Network([DenseLayer(3, 3), ReLU(), DenseLayer(input_size, output_size, W_init=W+delta, b_init=b), ReLU(),DenseLayer(4, 4), ReLU(), Softmax()], loss=CrossEntropy())\n",
    "            net2 = Network([DenseLayer(3, 3), ReLU(), DenseLayer(input_size, output_size, W_init=W-delta, b_init=b), ReLU(),DenseLayer(4, 4), ReLU(), Softmax()], loss=CrossEntropy())\n",
    "            diff = (net1.calculate_loss(x, l) - net2.calculate_loss(x, l)) / (2*eps)\n",
    "            right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad():\n",
    "    input_size = 3\n",
    "    output_size = 4 \n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((4,))\n",
    "    x = np.random.random((2, input_size))\n",
    "    l = np.array([[0,0,1,0],[0,0,0,1]])    \n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = Network([DenseLayer(3, 3), ReLU(), dense, ReLU(), DenseLayer(4, 4), ReLU(), Softmax()], loss=CrossEntropy()).grad_param(x, l)[2][0]\n",
    "\n",
    "    num_grad = numerical_grad(input_size, output_size, b_init, W_init, x, l)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "\n",
    "test_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def my_init(shape, dtype=None):\n",
    "    kernel = np.load('kernel.npy')\n",
    "    return kernel\n",
    "def my_initb(shape, dtype=None):\n",
    "    kernel = np.load('bias.npy')\n",
    "    return kernel\n",
    "tens = np.load('tensor.npy')\n",
    "kernel = np.load('kernel.npy')\n",
    "bias = np.load('bias.npy')\n",
    "layer = Conv2DLayer(kernel_size=3, input_channels=5, output_channels=2, \n",
    "                 padding='valid', stride=2, K_init=kernel.transpose((3,2,0,1)), b_init=bias)\n",
    "res = layer.forward(tens.transpose((0,3,1,2)))\n",
    "resk = Conv2D(2, 3, strides=2, padding=\"valid\", kernel_initializer=my_init, bias_initializer=my_initb, input_shape=tens.shape[1:])(tens)\n",
    "if np.sum(np.abs(res.transpose((0, 2, 3, 1)) - resk)) < 0.001:\n",
    "        print('Test PASSED')\n",
    "layer = Conv2DLayer(kernel_size=3, input_channels=5, output_channels=2, \n",
    "                 padding='same', stride=1, K_init=kernel.transpose((3,2,0,1)), b_init=bias)\n",
    "res = layer.forward(tens.transpose((0,3,1,2)))\n",
    "resk = Conv2D(2, 3, strides=1, padding=\"same\", kernel_initializer=my_init, bias_initializer=my_initb, input_shape=tens.shape[1:])(tens)\n",
    "if np.sum(np.abs(res.transpose((0, 2, 3, 1)) - resk)) < 0.01:\n",
    "        print('Test PASSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def my_init(shape, dtype=None):\n",
    "    kernel = np.load('kernel.npy')\n",
    "    return kernel\n",
    "def my_initb(shape, dtype=None):\n",
    "    kernel = np.load('bias.npy')\n",
    "    return kernel\n",
    "tens = np.load('tensor.npy')\n",
    "kernel = np.load('kernel.npy')\n",
    "bias = np.load('bias.npy')\n",
    "layer = Conv2DLayer1(kernel_size=3, input_channels=5, stride=2, output_channels=2, K_init=kernel.transpose((3,2,0,1)), b_init=bias)\n",
    "res = layer.forward(tens.transpose((0,3,1,2)))\n",
    "resk = Conv2D(2, 3, strides=2, padding=\"valid\", kernel_initializer=my_init, bias_initializer=my_initb, input_shape=tens.shape[1:])(tens)\n",
    "if np.sum(np.abs(res.transpose((0, 2, 3, 1)) - resk)) < 0.001:\n",
    "        print('Test PASSED')\n",
    "layer = Conv2DLayer(kernel_size=3, input_channels=5, output_channels=2, \n",
    "                 padding='same', stride=1, K_init=kernel.transpose((3,2,0,1)), b_init=bias)\n",
    "res = layer.forward(tens.transpose((0,3,1,2)))\n",
    "resk = Conv2D(2, 3, strides=1, padding=\"same\", kernel_initializer=my_init, bias_initializer=my_initb, input_shape=tens.shape[1:])(tens)\n",
    "if np.sum(np.abs(res.transpose((0, 2, 3, 1)) - resk)) < 0.01:\n",
    "        print('Test PASSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 16, 72)\n",
      "(2, 16, 72)\n",
      "Test PASSED\n",
      "(2, 36, 72)\n",
      "(2, 36, 72)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_layert(layer, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            for k in range(x.shape[3]):\n",
    "                delta = np.zeros((x.shape[1], x.shape[2], x.shape[3]))\n",
    "                delta[i][j][k] = eps\n",
    "                diff = (layer.forward(x + delta) - layer.forward(x-delta)) / (2*eps)\n",
    "                right_answer.append(diff.reshape(2,-1).T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layert(layer, cin):\n",
    "    x = np.arange(72*cin).reshape((2, cin, 6, 6))\n",
    "    num_grad = numerical_diff_layert(layer, x)\n",
    "    grad = layer.grad_x(x)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad-grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad[0])\n",
    "        print('Your gradient is ')\n",
    "        print(grad[0])\n",
    "        print(np.max(num_grad[0]-grad[0]))\n",
    "layer = Conv2DLayer(padding='valid', stride=1, input_channels=2, output_channels=1)\n",
    "test_layert(layer, 2)\n",
    "layer = Conv2DLayer(padding='same', stride=1, input_channels=2, output_channels=1)\n",
    "test_layert(layer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 72, 72)\n",
      "(2, 72, 72)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = FlattenLayer()\n",
    "test_layert(layer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 18, 72)\n",
      "(2, 18, 72)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = MaxPooling()\n",
    "test_layert(layer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 72, 72)\n",
      "(2, 72, 72)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = ReLU()\n",
    "test_layert(layer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32, 2)\n",
      "(2, 32, 2)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_layertb(layer, x):\n",
    "    eps = 0.00001\n",
    "    kernel = np.ones((2,2,3,3))\n",
    "    bias = np.arange(2)\n",
    "    right_answer = []\n",
    "    for i in range(len(bias)):\n",
    "        delta = np.zeros(len(bias))\n",
    "        delta[i] = eps\n",
    "        diff = (Conv2DLayer(padding='valid', stride=1, input_channels=2, output_channels=2, K_init=kernel, b_init=bias+delta)\\\n",
    "                .forward(x) - Conv2DLayer(padding='valid', stride=1, input_channels=2, \\\n",
    "                                           output_channels=2, K_init=kernel, b_init=bias-delta)\\\n",
    "                .forward(x)) / (2*eps)\n",
    "        right_answer.append(diff.reshape(2,-1).T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layertb(layer):\n",
    "    x = np.arange(144).reshape((2, 2, 6, 6))\n",
    "    num_grad = numerical_diff_layertb(layer, x)\n",
    "    grad = layer.grad_bias(x)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad-grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad[0])\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "kernel = np.ones((2,2,3,3))\n",
    "bias = np.arange(2)\n",
    "layer = Conv2DLayer(padding='valid', stride=1, input_channels=2, output_channels=2, K_init=kernel, b_init=bias)\n",
    "test_layertb(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32, 36)\n",
      "(2, 32, 36)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_layertk(layer, x):\n",
    "    eps = 0.00001\n",
    "    kernel = np.ones((2,2,3,3))\n",
    "    bias = np.arange(2)\n",
    "    right_answer = []\n",
    "    for k in range(kernel.shape[0]):\n",
    "        for l in range(kernel.shape[1]):\n",
    "            for i in range(kernel.shape[2]):\n",
    "                for j in range(kernel.shape[3]):\n",
    "                    delta = np.zeros(kernel.shape)\n",
    "                    delta[k][l][i][j] = eps\n",
    "                    diff = (Conv2DLayer(padding='valid', stride=1,\\\n",
    "                                        input_channels=2, output_channels=2, K_init=kernel+delta, b_init=bias)\\\n",
    "                            .forward(x) - Conv2DLayer(padding='valid', stride=1, input_channels=2, \\\n",
    "                                                              output_channels=2, K_init=kernel-delta, b_init=bias)\\\n",
    "                            .forward(x)) / (2*eps)\n",
    "                    right_answer.append(diff.reshape(2,-1).T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layertk(layer):\n",
    "    x = np.arange(144).reshape((2, 2, 6, 6))\n",
    "    num_grad = numerical_diff_layertk(layer, x)\n",
    "    grad = layer.grad_kernel(x)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad-grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad[0][0])\n",
    "        print('Your gradient is ')\n",
    "        print(grad[0][0])\n",
    "kernel = np.ones((2,2,3,3))\n",
    "bias = np.arange(2)\n",
    "layer = Conv2DLayer(padding='valid', stride=1, input_channels=2, output_channels=2, K_init=kernel, b_init=bias)\n",
    "test_layertk(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 242, 72)\n",
      "(2, 242, 72)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = Conv2DTrLayer(padding=1, stride=2, input_channels=2, output_channels=2)\n",
    "test_layert(layer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 98, 2)\n",
      "(2, 98, 2)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_layertb1(layer, x):\n",
    "    eps = 0.00001\n",
    "    kernel = np.ones((3,3,2,2))\n",
    "    bias = np.arange(2)\n",
    "    right_answer = []\n",
    "    for i in range(len(bias)):\n",
    "        delta = np.zeros(len(bias))\n",
    "        delta[i] = eps\n",
    "        diff = (Conv2DTrLayer(padding=1, stride=2, input_channels=2, output_channels=2, K_init=kernel, b_init=bias+delta)\\\n",
    "                .forward(x) - Conv2DTrLayer(padding=1, stride=2, input_channels=2, output_channels=2, \\\n",
    "                                                    K_init=kernel, b_init=bias-delta)\\\n",
    "                .forward(x)) / (2*eps)\n",
    "        right_answer.append(diff.reshape(2,-1).T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layertb1(layer):\n",
    "    x = np.arange(64).reshape((2, 2, 4, 4))\n",
    "    num_grad = numerical_diff_layertb1(layer, x)\n",
    "    grad = layer.grad_bias(x)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad-grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "kernel = np.ones((3,3,2,2))\n",
    "bias = np.arange(2)\n",
    "layer = Conv2DTrLayer(padding=1, stride=2, input_channels=2, output_channels=2, K_init=kernel, b_init=bias)\n",
    "test_layertb1(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 98, 36)\n",
      "(2, 98, 36)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_diff_layertk1(layer, x):\n",
    "    eps = 0.00001\n",
    "    kernel = np.ones((3,3,2,2))\n",
    "    bias = np.arange(2)\n",
    "    right_answer = []\n",
    "    for k in range(kernel.shape[3]):\n",
    "        for l in range(kernel.shape[2]):\n",
    "            for i in range(kernel.shape[0]):\n",
    "                for j in range(kernel.shape[1]):\n",
    "                    delta = np.zeros(kernel.shape)\n",
    "                    delta[i][j][l][k] = eps\n",
    "                    diff = (Conv2DTrLayer(padding=1, stride=2,\\\n",
    "                                        input_channels=2, output_channels=2, K_init=kernel+delta, b_init=bias)\\\n",
    "                            .forward(x) - Conv2DTrLayer(padding=1, stride=2, input_channels=2, \\\n",
    "                                                              output_channels=2, K_init=kernel-delta, b_init=bias)\\\n",
    "                            .forward(x)) / (2*eps)\n",
    "                    right_answer.append(diff.reshape(2,-1).T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_layertk1(layer):\n",
    "    x = np.arange(64).reshape((2, 2, 4, 4))\n",
    "    num_grad = numerical_diff_layertk1(layer, x)\n",
    "    grad = layer.grad_kernel(x)\n",
    "    print(num_grad.shape)\n",
    "    print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad-grad)) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad[0][1])\n",
    "        print('Your gradient is ')\n",
    "        print(grad[0][1])\n",
    "kernel = np.ones((3,3,2,2))\n",
    "bias = np.arange(2)\n",
    "layer = Conv2DTrLayer(padding=1, stride=2, input_channels=2, output_channels=2, K_init=kernel, b_init=bias)\n",
    "test_layertk1(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Полностью реализуйте метод обратного распространения ошибки в функции train_step класса Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекомендуем реализовать сначала функцию Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
    "После чего, имея список градиентов, написать функцию обновления параметров для каждого слоя. \n",
    "\n",
    "Совет: рекомендуем написать тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_nW(input_size, output_size, b, W, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            delta = np.zeros(W.shape)\n",
    "            delta[i, j] = eps\n",
    "            dense1 = Network([FlattenLayer(), DenseLayer(input_size, output_size), \\\n",
    "                              ReLU(), DenseLayer(20, 10, W_init=W+delta, b_init=b), Softmax()], loss=CrossEntropy())\n",
    "            dense2 = Network([FlattenLayer(), DenseLayer(input_size, output_size), \\\n",
    "                              ReLU(), DenseLayer(20, 10, W_init=W-delta, b_init=b), Softmax()], loss=CrossEntropy())\n",
    "            diff = (dense1.calculate_loss(x, labels) - dense2.calculate_loss(x, labels)) / (2*eps)\n",
    "            right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_nW():\n",
    "    input_size = 784\n",
    "    output_size = 20 \n",
    "    #W_init = np.random.random((input_size, output_size))\n",
    "    #b_init = np.random.random((output_size))\n",
    "    W_init = np.random.random((20, 10))\n",
    "    b_init = np.random.random((10))\n",
    "    x = X_train[:3]\n",
    "    labels = Y_train[:3]    \n",
    "    dense = Network([FlattenLayer(), DenseLayer(input_size, output_size), \\\n",
    "                     ReLU(), DenseLayer(20, 10, W_init, b_init), Softmax()], loss=CrossEntropy())\n",
    "    grad = dense.grad_param(x, labels)\n",
    "\n",
    "    num_grad = numerical_grad_nW(input_size, output_size, b_init, W_init, x, labels)\n",
    "    #print(num_grad.shape)\n",
    "    #print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad[3][0])) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad)\n",
    "\n",
    "test_grad_nW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_nW1(input_size, output_size, b, W, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for k in range(W.shape[0]):\n",
    "        for l in range(W.shape[1]):\n",
    "            for i in range(W.shape[2]):\n",
    "                for j in range(W.shape[3]):\n",
    "                    delta = np.zeros(W.shape)\n",
    "                    delta[k, l, i, j] = eps\n",
    "                    dense1 = Network([Conv2DLayer(padding='valid', stride=1, input_channels=1, output_channels=1, K_init=W+delta, b_init=b),\\\n",
    "                                      FlattenLayer(), DenseLayer(input_size, output_size), \\\n",
    "                                      ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "                    dense2 = Network([Conv2DLayer(padding='valid', stride=1, input_channels=1, output_channels=1, K_init=W-delta, b_init=b),\\\n",
    "                                      FlattenLayer(), DenseLayer(input_size, output_size), \\\n",
    "                                      ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "                    diff = (dense1.calculate_loss(x, labels) - dense2.calculate_loss(x, labels)) / (2*eps)\n",
    "                    right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "def test_grad_nW1():\n",
    "    input_size = 676\n",
    "    output_size = 20 \n",
    "    #W_init = np.random.random((input_size, output_size))\n",
    "    #b_init = np.random.random((output_size))\n",
    "    K_init = np.random.random((1,1,3,3))\n",
    "    b_init = np.random.random((1))\n",
    "    x = X_train[::12000]\n",
    "    labels = Y_train[::12000]    \n",
    "    dense = Network([Conv2DLayer(padding='valid', stride=1, input_channels=1, output_channels=1, K_init=K_init, b_init=b_init),\\\n",
    "                     FlattenLayer(), DenseLayer(input_size, output_size), \\\n",
    "                     ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "    grad = dense.grad_param(x, labels)\n",
    "\n",
    "    num_grad = numerical_grad_nW1(input_size, output_size, b_init, K_init, x, labels)\n",
    "    #print(num_grad.shape)\n",
    "    #print(grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad[0][0])) < 0.01:\n",
    "        print('Test PASSED')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "        print('Numerical grad is')\n",
    "        print(num_grad)\n",
    "        print('Your gradient is ')\n",
    "        print(grad[0][0])\n",
    "\n",
    "test_grad_nW1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Ознакомьтесь с реализацией функции fit класса Network. Запустите обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 937/937 [02:06<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 937/937 [02:07<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 937/937 [02:08<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 937/937 [02:06<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 937/937 [02:07<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.85\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::3], Y_train[::3], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [03:05<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [03:08<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [03:03<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [03:01<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [03:03<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(trainX[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.001)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Продемонстрируйте, что ваша реализация позволяет обучать более глубокие нейронные сети "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [01:38<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [01:35<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [01:34<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [01:33<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [01:33<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.10\n"
     ]
    }
   ],
   "source": [
    "net = Network([Conv2DLayer(padding='valid', stride=1, input_channels=1, output_channels=1), ReLU(),\\\n",
    "               FlattenLayer(), DenseLayer(676, 10), Softmax()], loss=CrossEntropy())\n",
    "net.fit(X_train[::60], Y_train[::60], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:11<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:10<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:10<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:09<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:09<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.17\n"
     ]
    }
   ],
   "source": [
    "net = Network([Conv2DLayer(padding='same', stride=1, input_channels=1, output_channels=1), ReLU(),\\\n",
    "               FlattenLayer(), DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
    "net.fit(X_train[:600], Y_train[:600], validation_split=0.25, \n",
    "            batch_size=15, nb_epoch=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [14:58<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [14:05<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [14:05<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [13:56<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [13:52<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.14\n"
     ]
    }
   ],
   "source": [
    "net = Network([Conv2DLayer(padding='valid', stride=3, input_channels=1, output_channels=1), ReLU(),\\\n",
    "               Conv2DTrLayer(stride=2, input_channels=1, output_channels=1), ReLU(), \\\n",
    "               FlattenLayer(), DenseLayer(361, 10), ReLU(), DenseLayer(10, 10), Softmax()], loss=CrossEntropy())\n",
    "net.fit(X_train[::6], Y_train[::6], validation_split=0.25, \n",
    "            batch_size=16, nb_epoch=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
